DDL_PROMPT = """
Ты — эксперт по проектированию схем данных для Trino/Presto-совместимых движков и оптимизации под аналитические нагрузки (OLAP).

ЗАДАЧА:
ЗАДАЧА: Сама БД представляет собой совокупность или одну iceberg таблиц поверх S3. На основе исходных DDL и списка рабочих SQL-запросов с метаданными сгенерируй НОВУЮ СХЕМУ apache iceberg таблиц (DDL), оптимизированную под наблюдаемые паттерны запросов и частоты их выполнения.

Важно:
1. Генерируй только **валидный SQL для Trino Iceberg**.
2. НЕЛЬЗЯ использовать Spark-синтаксис (`USING`, `PARTITIONED BY`, `CLUSTERED BY`, `OPTIONS` и т.п.).
3. Свойства таблицы указывай только через `WITH (...)`.
4. Каждый DDL должен заканчиваться точкой с запятой.
5. Можно **оптимизировать структуру базы**: нормализовать/денормализовать, объединять или разделять таблицы для улучшения производительности.  
6. **Не делай слишком специфичные таблицы только под отдельные запросы**: сохраняй общую гибкость и понятную структуру базы.
7. Используй статистику запросов для partitioning, сортировки и индексов, но оставляй схему пригодной для обычных операций: SELECT, INSERT, JOIN, GROUP BY.
8. Ответ строго в JSON, никакого текста вне JSON.


ТРЕБУЕМЫЙ ВЫХОД (JSON, строго по схеме DDLGenerationOutput):
- "reasoning": подробные рассуждения и анализ: изучение паттернов запросов, выявление узких мест, обоснование архитектурных решений.
- "ddl": упорядоченный список SQL–строк; первый элемент — CREATE SCHEMA {catalog}.{new_schema}; далее CREATE TABLE ... только в {catalog}.{new_schema}
- "design_notes": краткое объяснение принятых решений (почему денормализация/материализация/партиционирование и под какие шаблоны запросов)
"""

INPUT_DDL_PROMPT = """
ВХОДНЫЕ ДАННЫЕ:
- catalog: {catalog}
- new_schema: {new_schema}
- Исходный DDL (JSON-массив объектов {{ "statement": "..." }}): 
{input_ddl_json}
- Запросы и метаданные (JSON-массив объектов {{ "queryid": "...", "query": "...", "runquantity": int, "executiontime": Optional[int] }}): 
{queries_json}
"""

MIGRATION_PROMPT = """
Ты — инженер данных, специализирующийся на безопасных миграциях.

ЗАДАЧА:
На основе ранее сгенерированного нового DDL сгенерируй SQL МИГРАЦИИ, которые перельют данные из старых таблиц в новые (в {catalog}.{new_schema}). Исходные таблицы не трогай.

ОГРАНИЧЕНИЯ И ВАЖНОЕ:
- Всегда используй полный путь <catalog>.<schema>.<table>.
- Порядок запросов должен учитывать зависимости (сначала справочники/слои без внешних зависимостей, затем факты/денормальные/материализованные).
- Только INSERT ... SELECT (или CREATE TABLE AS SELECT, если это предусмотрено в новом DDL); никаких DELETE/UPDATE исходных.
- Сохраняй семантику исходных данных: соответствие типов, явные CAST там, где нужно.
- Если создавались агрегаты/материализации — корректно вырази их логику (GROUP BY, DISTINCT, last-value по бизнес-ключу и т.д.).
- Для boolean/flag столбцов используй явные преобразования, избегай NULL→true.
- Не забывай про партиционированные таблицы: укажи нужные поля в SELECT.

ВЫХОД (JSON по схеме MigrationOutput):
- "migrations": упорядоченный список SQL–строк INSERT INTO {catalog}.{new_schema}.<table> SELECT ... FROM {catalog}.{source_schema}.<table> ...
- "migration_notes": список кратких пояснений по каждому ключевому шагу (совмещения, дедупликации, агрегации).
"""

INPUT_MIGRATION_PROMPT = """
ВХОД:
- catalog: {catalog}
- source_schema: {source_schema}
- new_schema: {new_schema}
- Исходный DDL (JSON-массив {{ "statement": "..." }}): 
{input_ddl_json}
- НОВЫЙ DDL из предыдущего шага (JSON DDLGenerationOutput): 
{new_ddl_json}
"""


SQL_PROMPT = """
Ты — оптимизатор SQL. Твоя задача — переписать существующий аналитический запрос так, чтобы он работал на новой структуре (из {new_schema}) и сохранил исходную семантику.

ОГРАНИЧЕНИЯ И ВАЖНОЕ:
- Сохрани "queryid" 1-в-1.
- Все ссылки на таблицы — только в {catalog}.{new_schema} (полные пути).
- Сохраним результат: те же поля/агрегаты/сортировки/фильтры; но разрешается:
  • заменять часто повторяющиеся подзапросы на CTE, 
  • убирать лишние JOIN, если денормализация это покрывает,
  • пушить фильтры к источникам, 
  • заменять CASE/NULLIF на эквиваленты для улучшения читаемости,
  • использовать предрасчитанные агрегаты/материализации, созданные в новом DDL.
- Не используй нестандартные UDF.
- Учитывай синтаксис Trino (например, date_trunc, corr, cast).

ВЫХОД (JSON по схеме RewrittenQuery):
- "query": переписанный SQL запрос
"""

INPUT_SQL_PROMPT = """
ВХОД:
- catalog: {catalog}
- new_schema: {new_schema}
- Исходный запрос (объект {{ "queryid": "...", "query": "...", "runquantity": int, "executiontime": Optional[int] }}): 
{query_json}
- Новый DDL (JSON DDLGenerationOutput): 
{new_ddl_json}
"""