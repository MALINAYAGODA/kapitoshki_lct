# === Kafka Consumer (–æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å) ===
# worker.py

import asyncio
import json
import logging
import re
import sys
from typing import Any, Optional

from aiokafka import AIOKafkaConsumer
from psycopg2.pool import ThreadedConnectionPool
from pydantic import BaseModel

from src.config import Config
from src.get_counts import get_counts
from src.models import DDLGenerationOutput, MigrationOutput, RewrittenQuery
from src.prompts import (
    DDL_PROMPT,
    INPUT_DDL_PROMPT,
    INPUT_MIGRATION_PROMPT,
    INPUT_SQL_PROMPT,
    MIGRATION_PROMPT,
    SQL_PROMPT,
)

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≤–æ—Ä–∫–µ—Ä–∞ ===


def setup_worker_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≤–æ—Ä–∫–µ—Ä–∞"""
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - [WORKER] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π —Ö–µ–Ω–¥–ª–µ—Ä
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)

    # –§–∞–π–ª–æ–≤—ã–π —Ö–µ–Ω–¥–ª–µ—Ä –¥–ª—è –≤–æ—Ä–∫–µ—Ä–∞
    file_handler = logging.FileHandler("worker.log", encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)

    # –£–º–µ–Ω—å—à–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
    logging.getLogger("aiokafka").setLevel(logging.WARNING)
    logging.getLogger("kafka").setLevel(logging.WARNING)
    logging.getLogger("openai").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING)

    return logging.getLogger(__name__)


worker_logger = setup_worker_logging()


class LLMProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM —á–µ—Ä–µ–∑ AsyncOpenAI"""

    model_name = "gpt-4.1"

    def __init__(self, api_key: str = None):
        from openai import AsyncOpenAI

        if api_key is None:
            api_key = "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"

        self.client = AsyncOpenAI(api_key=api_key)
        self.logger = logging.getLogger(self.__class__.__name__)

    def parse_catalog_and_schema_from_ddl(
        self,
        ddl_statements: list[dict[str, Any]],
    ) -> tuple[Optional[str], Optional[str]]:
        if not ddl_statements:
            return None, None

        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ CREATE TABLE catalog.schema.table_name
        pattern = r"CREATE\s+TABLE\s+([^.\s]+)\.([^.\s]+)\.([^.\s\(]+)"

        for ddl_item in ddl_statements:
            statement = ddl_item.get("statement", "")
            match = re.search(pattern, statement, re.IGNORECASE)
            if match:
                catalog = match.group(1)
                schema = match.group(2)
                self.logger.info(f"–ù–∞–π–¥–µ–Ω –∫–∞—Ç–∞–ª–æ–≥: '{catalog}', —Å—Ö–µ–º–∞: '{schema}'")
                return catalog, schema

        self.logger.info("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∫–∞—Ç–∞–ª–æ–≥ –∏ —Å—Ö–µ–º—É –≤ DDL statements")
        return None, None

    def get_database_config_from_data(
        self, data: dict[str, Any], new_schema: str = "optimized"
    ) -> dict[str, str]:
        ddl_statements = data.get("ddl", [])
        catalog, source_schema = self.parse_catalog_and_schema_from_ddl(ddl_statements)

        return {
            "catalog": catalog or "unknown",
            "source_schema": source_schema or "public",
            "new_schema": new_schema,
        }

    def count_tokens(self, text: str) -> int:
        """
        –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        –î–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ tiktoken, –∑–¥–µ—Å—å –ø—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞
        """
        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞: ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        # –î–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ tiktoken: pip install tiktoken
        try:
            import tiktoken

            encoding = tiktoken.encoding_for_model("gpt-4")
            return len(encoding.encode(text))
        except ImportError:
            # –ì—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞ –µ—Å–ª–∏ tiktoken –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            return len(text) // 4

    def calculate_tokens(
        self, system_prompt: str, user_prompt: str, output_data: Any
    ) -> tuple[int, int]:
        """–ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö –∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        input_tokens = self.count_tokens(system_prompt) + self.count_tokens(user_prompt)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ JSON —Å—Ç—Ä–æ–∫—É
        if hasattr(output_data, "model_dump"):
            output_text = json.dumps(output_data.model_dump(), ensure_ascii=False)
        else:
            output_text = json.dumps(output_data, ensure_ascii=False)

        output_tokens = self.count_tokens(output_text)

        return input_tokens, output_tokens

    async def make_openai_request(
        self,
        # model_name: str,
        system_prompt: str,
        user_prompt: str,
        response_schema: BaseModel,
        schema_name: str,
        temperature: float = 0,
        seed: int = 42,
    ) -> Any:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ OpenAI API
        """
        self.logger.info(
            f"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ OpenAI API (model={self.model_name}, temperature={temperature})"
        )
        try:
            response = await self.client.chat.completions.create(
                model=self.model_name,
                temperature=temperature,
                seed=seed,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={
                    "type": "json_schema",
                    "json_schema": {
                        "name": schema_name,
                        "schema": response_schema.model_json_schema(),
                    },
                },
            )

            result = response_schema.model_validate_json(
                response.choices[0].message.content
            )

            # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤
            input_tokens, output_tokens = self.calculate_tokens(
                system_prompt, user_prompt, result
            )
            self.logger.info(
                f"–ó–∞–ø—Ä–æ—Å –∫ OpenAI API –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ. "
                f"–¢–æ–∫–µ–Ω—ã: –≤—Ö–æ–¥={input_tokens:,}, –≤—ã—Ö–æ–¥={output_tokens:,}"
            )

            return result

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –∫ OpenAI API: {e}")
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –∫ OpenAI API: {e}")

    async def process_queries_batch(
        self,
        system_prompt: str,
        queries: list[dict[str, Any]],
        user_prompt_template: str,
        response_schema: BaseModel,
        schema_name: str,
        additional_data: Optional[dict[str, Any]] = None,
    ) -> tuple[list[dict[str, Any]], int, int]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        processed_queries = []
        total_input_tokens = 0
        total_output_tokens = 0

        self.logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤...")

        for i, query_data in enumerate(queries, 1):
            self.logger.info(
                f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {i}/{len(queries)}: {query_data['queryid']}"
            )

            try:
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —à–∞–±–ª–æ–Ω–∞
                template_data = {
                    "query_json": json.dumps(query_data, ensure_ascii=False)
                }
                if additional_data:
                    template_data.update(additional_data)

                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
                user_prompt = user_prompt_template.format(**template_data)

                # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ API
                result = await self.make_openai_request(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    response_schema=response_schema,
                    schema_name=schema_name,
                )

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å queryid –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                processed_result = {
                    "queryid": query_data["queryid"],
                    "query": result.query if hasattr(result, "query") else result,
                }
                processed_queries.append(processed_result)

                # –°—á–∏—Ç–∞–µ–º —Ç–æ–∫–µ–Ω—ã
                input_tokens, output_tokens = self.calculate_tokens(
                    system_prompt, user_prompt, result
                )
                total_input_tokens += input_tokens
                total_output_tokens += output_tokens

                self.logger.info(
                    f"–ó–∞–ø—Ä–æ—Å {query_data['queryid']} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω. "
                    f"–¢–æ–∫–µ–Ω—ã: –≤—Ö–æ–¥={input_tokens:,}, –≤—ã—Ö–æ–¥={output_tokens:,}"
                )

            except Exception as e:
                self.logger.error(
                    f"–û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞ {query_data['queryid']}: {e}",
                    exc_info=True,
                )
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å —Å –æ—à–∏–±–∫–æ–π –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                processed_queries.append(
                    {
                        "queryid": query_data["queryid"],
                        "query": f"-- –û–®–ò–ë–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò: {e}\n{query_data.get('query', '')}",
                        "error": str(e),
                    }
                )

        self.logger.info(
            f"–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. "
            f"–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: –≤—Ö–æ–¥={total_input_tokens:,}, –≤—ã—Ö–æ–¥={total_output_tokens:,}"
        )

        return processed_queries, total_input_tokens, total_output_tokens

    async def create_new_ddl(self, body: dict[Any], db_config) -> DDLGenerationOutput:
        input_ddl_json = json.dumps(body.get("ddl", []), ensure_ascii=False)
        queries_json = json.dumps(body.get("queries", []), ensure_ascii=False)

        tables = []
        for ddl in body.get("ddl"):
            tables.append(ddl["statement"].split()[2])

        counts = get_counts(body.get("url"), tables)
        self.logger.info("Got counts from all tables")

        system_prompt = DDL_PROMPT
        user_prompt = INPUT_DDL_PROMPT.format(
            catalog=db_config["catalog"],
            source_schema=db_config["source_schema"],
            new_schema=db_config["new_schema"],
            input_ddl_json=input_ddl_json,
            queries_json=queries_json,
            stats=counts,
        )

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ OpenAI API
        ddl_out = await self.make_openai_request(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=DDLGenerationOutput,
            schema_name="ddl_generation_output",
        )

        return ddl_out

    async def create_migrations(
        self, ddl_output: DDLGenerationOutput, body: dict[Any], db_config
    ) -> MigrationOutput:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –º–∏–≥—Ä–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ–≥–æ DDL
        """
        input_ddl_json = json.dumps(body.get("ddl", []), ensure_ascii=False)
        new_ddl_json = json.dumps(ddl_output.ddl, ensure_ascii=False)

        system_prompt = MIGRATION_PROMPT
        user_prompt = INPUT_MIGRATION_PROMPT.format(
            catalog=db_config["catalog"],
            source_schema=db_config["source_schema"],
            new_schema=db_config["new_schema"],
            input_ddl_json=input_ddl_json,
            new_ddl_json=new_ddl_json,
        )

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ OpenAI API
        migration_out = await self.make_openai_request(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=MigrationOutput,
            schema_name="migration_output",
        )

        return migration_out

    async def create_queries(
        self, ddl_output: DDLGenerationOutput, body: dict[Any], db_config
    ):
        queries = body.get("queries", [])
        new_ddl_json = json.dumps(ddl_output.ddl, ensure_ascii=False)

        system_prompt = SQL_PROMPT.format(
            catalog=db_config["catalog"], new_schema=db_config["new_schema"]
        )

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã —Å –ø–æ–º–æ—â—å—é —É—Ç–∏–ª–∏—Ç—ã
        (
            rewritten_queries,
            total_input_tokens,
            total_output_tokens,
        ) = await self.process_queries_batch(
            system_prompt=system_prompt,
            queries=queries,
            user_prompt_template=INPUT_SQL_PROMPT,
            response_schema=RewrittenQuery,
            schema_name="rewritten_query",
            additional_data={
                "catalog": db_config["catalog"],
                "new_schema": db_config["new_schema"],
                "new_ddl_json": new_ddl_json,
            },
        )

        return rewritten_queries

    async def refactor_db_schema(self, body: dict[Any]):
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –ë–î —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM
        """

        db_config = self.get_database_config_from_data(body)

        self.logger.info("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ DDL –∑–∞–ø—Ä–æ—Å—ã")
        ddl = await self.create_new_ddl(body, db_config)

        self.logger.info("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏")
        migrations = await self.create_migrations(ddl, body, db_config)

        self.logger.info("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –Ω–æ–≤–æ–π —Å—Ö–µ–º—ã –¥–∞–Ω–Ω—ã—Ö")
        queries = await self.create_queries(ddl, body, db_config)

        return {
            "ddl": ddl.model_dump(),
            "migrations": migrations.model_dump(),
            "queries": queries,
        }


# async def main():
#     processor = LLMProcessor()
#     with open("./data/flights.json", "r", encoding="utf-8") as f:
#         task_data = json.load(f)
#     result = await processor.refactor_db_schema(task_data)
#     result_str = json.dumps(result, ensure_ascii=False)
#     print(result_str)


# if __name__ == "__main__":
#     asyncio.run(main())


async def process_task(task_data: dict, db_pool: ThreadedConnectionPool):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏
    """
    task_id = task_data["task_id"]

    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ processing
        conn = db_pool.getconn()
        try:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    UPDATE tasks 
                    SET status = %s, updated_at = NOW()
                    WHERE task_id = %s
                """,
                    ("processing", task_id),
                )
                conn.commit()
        finally:
            db_pool.putconn(conn)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ LLM
        processor = LLMProcessor()
        result = await processor.refactor_db_schema(task_data["body"])

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        conn = db_pool.getconn()
        try:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    UPDATE tasks 
                    SET status = %s, result_data = %s, progress = %s, updated_at = NOW()
                    WHERE task_id = %s
                """,
                    ("completed", json.dumps(result, ensure_ascii=False), 100, task_id),
                )
                conn.commit()
        finally:
            db_pool.putconn(conn)

        worker_logger.info(f"Task {task_id} completed successfully")

    except Exception as e:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—à–∏–±–∫—É
        conn = db_pool.getconn()
        try:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    UPDATE tasks 
                    SET status = %s, error_message = %s, updated_at = NOW()
                    WHERE task_id = %s
                """,
                    ("failed", str(e), task_id),
                )
                conn.commit()
        finally:
            db_pool.putconn(conn)

        worker_logger.info(f"Task {task_id} failed: {str(e)}")


async def kafka_worker():
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ—Ä–∫–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ Kafka
    """
    worker_logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–æ—Ä–∫–µ—Ä–∞...")

    # Kafka consumer (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π) —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    consumer = AIOKafkaConsumer(
        Config.KAFKA_TOPIC,
        bootstrap_servers=Config.KAFKA_BOOTSTRAP_SERVERS,
        value_deserializer=lambda m: json.loads(m.decode("utf-8")),
        group_id="llm_workers",
        auto_offset_reset="earliest",
        enable_auto_commit=True,
        consumer_timeout_ms=2000,
    )

    worker_logger.info(
        f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Kafka (bootstrap_servers={Config.KAFKA_BOOTSTRAP_SERVERS})..."
    )
    worker_logger.info(f"–ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ —Ç–æ–ø–∏–∫: {Config.KAFKA_TOPIC}, –≥—Ä—É–ø–ø–∞: llm_workers")

    max_retries = 10
    retry_delay = 10

    for attempt in range(1, max_retries + 1):
        try:
            await consumer.start()
            worker_logger.info("‚úÖ Kafka Consumer —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω –∏ –∑–∞–ø—É—â–µ–Ω")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–µ –ø–∞—Ä—Ç–∏—Ü–∏–∏
            await asyncio.sleep(2)  # –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∫ –≥—Ä—É–ø–ø–µ
            partitions = consumer.assignment()
            worker_logger.info(f"–ù–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–µ –ø–∞—Ä—Ç–∏—Ü–∏–∏: {partitions}")

            break
        except Exception as e:
            worker_logger.error(
                f"‚ùå –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries} –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Kafka –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}"
            )
            if attempt < max_retries:
                worker_logger.info(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {retry_delay} —Å–µ–∫—É–Ω–¥...")
                await asyncio.sleep(retry_delay)
            else:
                worker_logger.error(
                    "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Kafka –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫"
                )
                raise

    worker_logger.info(
        f"üöÄ –í–æ—Ä–∫–µ—Ä –∑–∞–ø—É—â–µ–Ω, –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á –∏–∑ —Ç–æ–ø–∏–∫–∞ '{Config.KAFKA_TOPIC}'..."
    )

    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
    try:
        db_pool = ThreadedConnectionPool(
            Config.DB_POOL_MIN,
            Config.DB_POOL_MAX,
            host=Config.POSTGRES_HOST,
            port=Config.POSTGRES_PORT,
            database=Config.POSTGRES_DB,
            user=Config.POSTGRES_USER,
            password=Config.POSTGRES_PASSWORD,
        )
        worker_logger.info("–ü—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —Å –ë–î —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        worker_logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —Å –ë–î: {e}")
        raise

    try:
        message_count = 0
        async for message in consumer:
            message_count += 1
            task_data = message.value
            worker_logger.info(
                f"üì• –ü–æ–ª—É—á–µ–Ω–∞ –∑–∞–¥–∞—á–∞ #{message_count}: {task_data['task_id']} "
                f"(partition={message.partition}, offset={message.offset})"
            )
            await process_task(task_data, db_pool)
    except KeyboardInterrupt:
        worker_logger.info("–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ (Ctrl+C)")
    except Exception as e:
        worker_logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –≤–æ—Ä–∫–µ—Ä–µ: {e}", exc_info=True)
    finally:
        worker_logger.info("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–æ—Ä–∫–µ—Ä–∞...")
        await consumer.stop()
        db_pool.closeall()
        worker_logger.info("–í–æ—Ä–∫–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")


if __name__ == "__main__":
    asyncio.run(kafka_worker())
